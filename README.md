# Job Data ETL Project

This project is an Airflow ETL pipeline that extracts job listings from an API, transforms the data, and loads it into a PostgreSQL database. The tasks are isolated into separate modules within the `scripts/` directory, enhancing modularity and maintainability. The application runs within Docker containers for consistency and ease of deployment.

## Project Structure

- `dags/`: Contains the Airflow DAG definitions.
- `scripts/`: Contains modules for extraction, transformation, loading, and data models.
- `tests/`: Contains unit tests for each module.
- `config/`: Contains configuration files.
- `logs/`: Stores logs generated by Airflow and the application.
- `data/`: Directory for any data files (if needed).
- `Dockerfile`: Docker image configuration.
- `docker-compose.yml`: Docker Compose configuration.
- `requirements.txt`: Python dependencies.
- `pyproject.toml`: Configuration for code formatting tools.
- `.pre-commit-config.yaml`: Pre-commit hooks configuration.
- `tasks.py`: Automation tasks using `invoke`.
- `README.md`: Project documentation.

## Getting Started

### Prerequisites

- Docker and Docker Compose
- Python 3.8
- Git

### Setup Instructions

1. **Clone the repository**:

   ```bash
   git clone https://github.com/yourusername/job_data_etl_project.git
   cd job_data_etl_project
